{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13856731,"sourceType":"datasetVersion","datasetId":8827269},{"sourceId":663020,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":501670,"modelId":516834}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#imports\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nfrom datasets import Dataset, load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, PeftModel, PeftConfig\n#from trl import SFTConfig, SFTTrainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:35:27.126108Z","iopub.execute_input":"2025-11-27T11:35:27.126460Z","iopub.status.idle":"2025-11-27T11:35:38.791756Z","shell.execute_reply.started":"2025-11-27T11:35:27.126435Z","shell.execute_reply":"2025-11-27T11:35:38.790548Z"}},"outputs":[{"name":"stderr","text":"2025-11-27 11:35:33.017938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764243333.040621     190 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764243333.047312     190 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## Preprocessing","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/annomi/\"\n\nannomi = pd.read_csv(data_path + \"AnnoMI-full.csv\")\nannomi = annomi.drop(['video_title', 'video_url', 'topic', 'timestamp', 'annotator_id', 'therapist_input_exists', 'therapist_input_subtype', 'reflection_exists', 'reflection_subtype', 'question_exists', 'question_subtype', 'main_therapist_behaviour', 'client_talk_type'], axis=1)\nannomi = annomi.query(\"mi_quality == 'high'\")\nannomi = annomi.drop_duplicates(subset=['transcript_id', 'utterance_id'])\nannomi = annomi.drop(['mi_quality'], axis=1) #, 'utterance_id'\n\n#cleaning unwanted characters\nannomi['utterance_text'] = annomi['utterance_text'].str.replace(r'\\[.*?\\]', '', regex=True)\nannomi['utterance_text'] = annomi['utterance_text'].str.replace(r'-', ' ', regex=True)\nannomi['utterance_text'] = annomi['utterance_text'].str.replace(r'â€”', ' ', regex=True)\n\n#making everything lower case and removing multiple spaces\nannomi['utterance_text'] = annomi['utterance_text'].str.lower()\nannomi['utterance_text'] = annomi['utterance_text'].str.strip().replace(r'\\s+', ' ', regex=True)\n\n#getting only client-therapist pairs of utterances\nfirst_utterances = annomi.groupby('transcript_id')['utterance_id'].transform('min') == annomi['utterance_id']\nlast_utterances = annomi.groupby('transcript_id')['utterance_id'].transform('max') == annomi['utterance_id']\nannomi = annomi[~((first_utterances & (annomi['interlocutor'] == 'therapist')) | (last_utterances & (annomi['interlocutor'] == 'client')))]\n\n#checks\nprint(annomi.head())\n#print(annomi.tail())\nprint(annomi.shape)\n#print(annomi.iloc[0])\n#pd.set_option('display.max_colwidth', None)\n#print(annomi[(annomi['transcript_id'] == 18) & (annomi['utterance_id'] == 91)]['utterance_text'])\nmax_length = annomi['utterance_text'].str.len().max()\nprint(f\"Max sentence length: {max_length} characters\")\n\n#count occurrences of each speaker type\nspeaker_counts = annomi['interlocutor'].value_counts()\nprint(speaker_counts)\ninterviewer_count = annomi['interlocutor'].value_counts()['therapist']\ninterviewed_count = annomi['interlocutor'].value_counts()['client']\nprint(f\"Therapist: {interviewer_count}\")\nprint(f\"Client: {interviewed_count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:35:51.159151Z","iopub.execute_input":"2025-11-27T11:35:51.160363Z","iopub.status.idle":"2025-11-27T11:35:51.333658Z","shell.execute_reply.started":"2025-11-27T11:35:51.160326Z","shell.execute_reply":"2025-11-27T11:35:51.332974Z"}},"outputs":[{"name":"stdout","text":"   transcript_id  utterance_id interlocutor  \\\n1              0             1       client   \n2              0             2    therapist   \n3              0             3       client   \n4              0             4    therapist   \n5              0             5       client   \n\n                                      utterance_text  \n1                                              sure.  \n2  so, let's see. it looks that you put you drink...  \n3                                            mm hmm.  \n4  and you usually have three to four drinks when...  \n5          usually three drinks and glasses of wine.  \n(8693, 4)\nMax sentence length: 1193 characters\ninterlocutor\ntherapist    4347\nclient       4346\nName: count, dtype: int64\nTherapist: 4347\nClient: 4346\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#putting examples in instruct format for mistral 7b\n\ntraining_examples = []\n\nfor transcript_id, group in annomi.groupby('transcript_id'):\n    \n    group = group.sort_values(by='utterance_id')\n    utterances = list(group[['interlocutor', 'utterance_text']].itertuples(index=False))\n    current_prompt = \"\"\n    \n    for interlocutor, text in utterances:\n        \n        if interlocutor.lower() == 'client':\n            current_prompt += text + \" \"\n        \n        elif interlocutor.lower() == 'therapist':\n\n            if current_prompt.strip():\n\n                formatted_turn = (\n                    \"<s>[INST] \" + \n                    current_prompt.strip() + \n                    \" [/INST] \" + \n                    text.strip() + \n                    \"</s>\"\n                )\n                training_examples.append(formatted_turn)\n                \n            current_prompt = \"\" \n            \nannomi_finetune = pd.DataFrame(training_examples, columns=['text'])\n\nprint(len(annomi_finetune))\nprint(annomi_finetune['text'].iloc[0])\nprint(annomi_finetune.head())\nprint(annomi_finetune.shape)\n\ndataset = Dataset.from_pandas(annomi_finetune)\nprint(dataset)\n\nprint(dataset[\"text\"][100])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:35:53.991356Z","iopub.execute_input":"2025-11-27T11:35:53.992431Z","iopub.status.idle":"2025-11-27T11:35:54.120822Z","shell.execute_reply.started":"2025-11-27T11:35:53.992395Z","shell.execute_reply":"2025-11-27T11:35:54.119899Z"}},"outputs":[{"name":"stdout","text":"4325\n<s>[INST] sure. [/INST] so, let's see. it looks that you put you drink alcohol at least four times a week on average</s>\n                                                text\n0  <s>[INST] sure. [/INST] so, let's see. it look...\n1  <s>[INST] mm hmm. [/INST] and you usually have...\n2  <s>[INST] usually three drinks and glasses of ...\n3  <s>[INST] something like that. [/INST] okay. j...\n4  <s>[INST] okay. [/INST] uh, what else can you ...\n(4325, 1)\nDataset({\n    features: ['text'],\n    num_rows: 4325\n})\n<s>[INST] yeah, no, it's easy. [/INST] yeah. yeah. you know, i've been thinking about, um, thinking about you, and thinking about, um, that first meeting we had, and where you are now. so, wh what do you think some of the changes are that that you've made in this in this, sort of, five five to six weeks that we've been meeting?</s>\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Finetuning","metadata":{}},{"cell_type":"code","source":"#configurations\n\n#bitsandbytes\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\n#lora\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n#trainer\ntrainer_config = TrainingArguments(\n    num_train_epochs=1,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=1,\n\n    optim=\"paged_adamw_32bit\",\n    learning_rate=1e-4,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"constant\",\n\n    max_grad_norm=0.3,\n    weight_decay=0.001,\n\n    bf16=True,\n    save_steps=100,\n    logging_steps=20,\n    output_dir=\"/kaggle/working/\",\n    report_to=\"none\",\n    gradient_checkpointing = False,\n    label_names=[\"labels\"]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:35:59.156182Z","iopub.execute_input":"2025-11-27T11:35:59.156966Z","iopub.status.idle":"2025-11-27T11:35:59.299271Z","shell.execute_reply.started":"2025-11-27T11:35:59.156939Z","shell.execute_reply":"2025-11-27T11:35:59.298661Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"'''#downloading model\nmodel_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.save_pretrained(\"/kaggle/working/mistral-7b-saved\")\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map=\"auto\", trust_remote_code=True, low_cpu_mem_usage=True)\nmodel.save_pretrained(\"/kaggle/working/mistral-7b-saved\")'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading model and tokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/mistral-7b-saved\", local_files_only=True)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\ntokenizer.add_bos_token = False\ntokenizer.add_eos_token = False\ntokenizer.model_max_length = 4096\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"/kaggle/working/mistral-7b-saved\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    local_files_only=True,\n    low_cpu_mem_usage=True\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#tokenizing the dataset\ndef tokenize_function(dialogues):\n    return tokenizer(dialogues[\"text\"], max_length=4096, truncation=True, padding=True)\n\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset.column_names,\n)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#training the model\ntrainer = Trainer(\n    model=model,\n    train_dataset=tokenized_dataset,\n    args=trainer_config,\n    data_collator = data_collator\n)\n\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#saving the model\ntrainer.model.save_pretrained(\"/kaggle/working/mistral-7b-annomi-adapter\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"from peft import PeftConfig\n\nadapter_config = PeftConfig.from_pretrained(\"/kaggle/input/mistral-7b-annomi-adapter/transformers/default/1\")\nprint('loaded')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#loading model for inference\nbase_model = AutoModelForCausalLM.from_pretrained(\n    #model_name,\n    \"/kaggle/working/mistral-7b-saved\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    local_files_only=True,\n    low_cpu_mem_usage=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/working/mistral-7b-saved\", local_files_only=True)\n\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\ntokenizer.add_bos_token = False\ntokenizer.add_eos_token = False\n\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/mistral-7b-annomi-adapter/transformers/default/1\")\nprint(model)\n#model.save_pretrained(\"/kaggle/working/mistral-7b-annomi-adapter\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:36:07.320148Z","iopub.execute_input":"2025-11-27T11:36:07.320477Z","iopub.status.idle":"2025-11-27T11:36:15.113267Z","shell.execute_reply.started":"2025-11-27T11:36:07.320452Z","shell.execute_reply":"2025-11-27T11:36:15.112271Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n  warnings.warn(warning_msg)\n","output_type":"stream"},{"name":"stdout","text":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32768, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4096, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): MistralMLP(\n              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): MistralRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n    )\n  )\n)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"#running prompts\nmodel.eval()\n\ngenerator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n\nprompt_text = \"I am having trouble getting out of bed\"\nformatted_prompt = f\"<s>[INST] {prompt_text} [/INST]\"\n\nresponse = generator(formatted_prompt, max_new_tokens=120, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:36:29.546330Z","iopub.execute_input":"2025-11-27T11:36:29.546637Z","iopub.status.idle":"2025-11-27T11:36:47.419919Z","shell.execute_reply.started":"2025-11-27T11:36:29.546615Z","shell.execute_reply":"2025-11-27T11:36:47.419049Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"[{'generated_text': \"<s>[INST] I am having trouble getting out of bed [/INST] mm hmm. and how long have you been having trouble getting out of bed? i'm curious about that. how long have you been having trouble getting out of bed? is this a new thing? how long have you been having trouble getting out of bed? how long has this issue lasted? what's the history there? and what have you done to get out of bed? what have you done to get yourself out of bed? what's your history with getting out of bed? so that i can understand your situation a little bit better. can you kind of give me a\"}]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"prompt_text = \"I have been drinking too much recently\"\nformatted_prompt = f\"<s>[INST] {prompt_text} [/INST]\"\n\nresponse = generator(formatted_prompt, max_new_tokens=200, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:39:07.394604Z","iopub.execute_input":"2025-11-27T11:39:07.395341Z","iopub.status.idle":"2025-11-27T11:39:34.892690Z","shell.execute_reply.started":"2025-11-27T11:39:07.395314Z","shell.execute_reply":"2025-11-27T11:39:34.891812Z"}},"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] I have been drinking too much recently [/INST] uh huh. and how has that been a problem for you? um, um, and can you tell me what you know about drinking during pregnancy? have you picked up any information? have you talked with anyone about this? um, what have you heard? what do you know about that? what's your knowledge base? do you have any ideas about drinking and pregnancy? what do you know? um, and what concerns do you have about that? what's your feeling about that? do you want to continue drinking during pregnancy? what would you like to do? how do you feel about that? how would you like to proceed? yeah, go ahead. go ahead, go ahead. so what is your knowledge base on this? where does your information come from? where do you have any concerns? what do you have any idea about, um, the impact of alcohol during pregnancy? um, and what do you think you would like to do? um, what would\"}]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"prompt_text = \"I am finding it quite hard to find motivation to go to work\"\nformatted_prompt = f\"<s>[INST] {prompt_text} [/INST]\"\n\nresponse = generator(formatted_prompt, max_new_tokens=50, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:41:43.820382Z","iopub.execute_input":"2025-11-27T11:41:43.821257Z","iopub.status.idle":"2025-11-27T11:41:51.290648Z","shell.execute_reply.started":"2025-11-27T11:41:43.821203Z","shell.execute_reply":"2025-11-27T11:41:51.289646Z"}},"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] I am finding it quite hard to find motivation to go to work [/INST] right. so what do you think is the reason for that? what do you think is the reason you're struggling to find motivation to go to work? what do you think is the reason why you're finding it hard to get up in\"}]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"system_prompt = \"You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\"\nprompt_text = \"I am having trouble getting out of bed\"\n\nformatted_prompt = f\"<s>[INST] {system_prompt} {prompt_text} [/INST]\"\n\nresponse = generator(formatted_prompt, max_new_tokens=120, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:49:06.902122Z","iopub.execute_input":"2025-11-27T11:49:06.902702Z","iopub.status.idle":"2025-11-27T11:49:23.548021Z","shell.execute_reply.started":"2025-11-27T11:49:06.902676Z","shell.execute_reply":"2025-11-27T11:49:23.547261Z"}},"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses. I am having trouble getting out of bed [/INST] it's a bit of a struggle then. you're not enjoying it, you're just not. what's going on? what's getting you out of bed? what motivates you to get out of bed? what's the good part of getting out of bed? what's the bad part of staying in bed? what's going on? what's going on, uh, with your mind? what's going on with your body? what's going on with your life? why do you wanna get out of bed? what's\"}]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"system_prompt = \"You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\"\nprompt_text = \"I have been drinking too much recently\"\n\nformatted_prompt = f\"<s>[INST] {system_prompt} {prompt_text} [/INST]\"\n\nresponse = generator(formatted_prompt, max_new_tokens=60, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:56:38.328712Z","iopub.execute_input":"2025-11-27T11:56:38.329359Z","iopub.status.idle":"2025-11-27T11:56:46.945324Z","shell.execute_reply.started":"2025-11-27T11:56:38.329332Z","shell.execute_reply":"2025-11-27T11:56:46.944457Z"}},"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses. I have been drinking too much recently [/INST] okay. so, i'm hearing that you've been drinking a little bit more than you normally do. and, i'm wondering what brought you here to talk about that. what what's going on that made you come in today? what's making you want to talk about\"}]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"system_prompt = \"You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\"\nprompt_text = \"I am finding it quite hard to find motivation to go to work\"\n\nformatted_prompt = f\"<s>[INST] {system_prompt} {prompt_text} [/INST]\"\n\nresponse = generator(formatted_prompt, max_new_tokens=50, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T11:53:57.681756Z","iopub.execute_input":"2025-11-27T11:53:57.682021Z","iopub.status.idle":"2025-11-27T11:54:04.971008Z","shell.execute_reply.started":"2025-11-27T11:53:57.682001Z","shell.execute_reply":"2025-11-27T11:54:04.970306Z"}},"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses. I am finding it quite hard to find motivation to go to work [/INST] yeah. you're just yeah, you're just thinking of all the things you don't want to do. um, it's definitely can be hard to find the motivation. so what would it take for you to go to work\"}]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"old_prompt = \"I am finding it quite hard to find motivation to go to work [/INST] yeah. you're just yeah, you're just thinking of all the things you don't want to do. um, it's definitely can be hard to find the motivation. so what would it take for you to go to work\"\nsystem_prompt = \"You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\"\nprompt_text = \"I think I would need to be treated with more respect and be paid better honestly\"\n\nformatted_prompt = f\"<s>[INST] {system_prompt}\\n\\n{old_prompt} </s>[INST] {prompt_text} [/INST]\"\nprint(formatted_prompt)\n\nresponse = generator(formatted_prompt, max_new_tokens=70, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:17:08.752106Z","iopub.execute_input":"2025-11-27T12:17:08.752935Z","iopub.status.idle":"2025-11-27T12:17:18.770515Z","shell.execute_reply.started":"2025-11-27T12:17:08.752909Z","shell.execute_reply":"2025-11-27T12:17:18.769760Z"}},"outputs":[{"name":"stdout","text":"<s>[INST] You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\n\nI am finding it quite hard to find motivation to go to work [/INST] yeah. you're just yeah, you're just thinking of all the things you don't want to do. um, it's definitely can be hard to find the motivation. so what would it take for you to go to work </s>[INST] I think I would need to be treated with more respect and be paid better honestly [/INST]\n[{'generated_text': \"<s>[INST] You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\\n\\nI am finding it quite hard to find motivation to go to work [/INST] yeah. you're just yeah, you're just thinking of all the things you don't want to do. um, it's definitely can be hard to find the motivation. so what would it take for you to go to work </s>[INST] I think I would need to be treated with more respect and be paid better honestly [/INST] okay. so what would be the first step for you to get to work? what would need to happen? and what would you need to do in order to get that to happen? what would need to happen? um, what would be the first step? what would be the second step? how would we get you on a path to being able\"}]\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"old_prompt = \"I am finding it quite hard to find motivation to go to work [/INST] yeah. you're just yeah, you're just thinking of all the things you don't want to do. um, it's definitely can be hard to find the motivation. so what would it take for you to go to work </s>[INST] I think I would need to be treated with more respect and be paid better honestly [/INST] okay. so what would be the first step for you to get to work? what would need to happen? and what would you need to do in order to get that to happen? what would need to happen? um, what would be the first step? what would be the second step? how would we get you on a path to being able\"\nsystem_prompt = \"You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\"\nprompt_text = \"Actually I am thinking of just quitting. I am very tired of everything\"\n\nformatted_prompt = f\"<s>[INST] {system_prompt}\\n\\n{old_prompt} </s>[INST] {prompt_text} [/INST]\"\n#print(formatted_prompt)\n\nresponse = generator(formatted_prompt, max_new_tokens=100, num_return_sequences=1, temperature=0.7, do_sample=True)\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-27T12:22:50.669346Z","iopub.execute_input":"2025-11-27T12:22:50.670126Z","iopub.status.idle":"2025-11-27T12:23:05.632763Z","shell.execute_reply.started":"2025-11-27T12:22:50.670102Z","shell.execute_reply":"2025-11-27T12:23:05.632076Z"}},"outputs":[{"name":"stdout","text":"[{'generated_text': \"<s>[INST] You are a helpful and empathetic mental health assistant that is a specialist in motivational interviewing. Provide supportive, thoughtful responses.\\n\\nI am finding it quite hard to find motivation to go to work [/INST] yeah. you're just yeah, you're just thinking of all the things you don't want to do. um, it's definitely can be hard to find the motivation. so what would it take for you to go to work </s>[INST] I think I would need to be treated with more respect and be paid better honestly [/INST] okay. so what would be the first step for you to get to work? what would need to happen? and what would you need to do in order to get that to happen? what would need to happen? um, what would be the first step? what would be the second step? how would we get you on a path to being able </s>[INST] Actually I am thinking of just quitting. I am very tired of everything [/INST] okay. so you're considering quitting your job. what would that look like to you? what would that mean? um, how would you go about quitting? you're a little tired of everything. you kind of want to go on vacation and just forget everything, and just start over. is that what you're thinking? how would that go? what what would that look like to you? what would you need to do? how would you do it? what would be\"}]\n","output_type":"stream"}],"execution_count":38}]}